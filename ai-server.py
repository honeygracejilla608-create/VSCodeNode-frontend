#!/usr/bin/env python3
"""
AI Development Server with GPU Support
Integrates with the Todo API for enhanced functionality
"""

import os
import asyncio
import logging
from typing import Dict, List, Optional, Any
import json

# AI/ML Libraries
import torch
import tensorflow as tf
import transformers
from transformers import pipeline
import cv2
import numpy as np

# API and Web Framework
from fastapi import FastAPI, HTTPException, BackgroundTasks
from fastapi.responses import JSONResponse
import uvicorn

# Google Cloud AI
from google.cloud import aiplatform
import vertexai
from vertexai.generative_models import GenerativeModel

# Configure logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class AIDevelopmentServer:
    def __init__(self):
        self.app = FastAPI(title="AI GPU Development Server", version="1.0.0")

        # Initialize GPU devices
        self.gpu_available = self._check_gpu_availability()

        # Load AI models
        self.models = {}
        self.pipelines = {}

        # Initialize AI services
        self._initialize_ai_models()

        # Setup API routes
        self._setup_routes()

    def _check_gpu_availability(self) -> Dict[str, Any]:
        """Check GPU availability and configuration"""
        gpu_info = {
            "cuda_available": torch.cuda.is_available(),
            "gpu_count": torch.cuda.device_count() if torch.cuda.is_available() else 0,
            "gpu_names": [],
            "memory_info": []
        }

        if gpu_info["cuda_available"]:
            for i in range(gpu_info["gpu_count"]):
                gpu_info["gpu_names"].append(torch.cuda.get_device_name(i))
                gpu_info["memory_info"].append({
                    "total": torch.cuda.get_device_properties(i).total_memory,
                    "reserved": torch.cuda.memory_reserved(i),
                    "allocated": torch.cuda.memory_allocated(i)
                })

        logger.info(f"GPU Status: {gpu_info}")
        return gpu_info

    def _initialize_ai_models(self):
        """Initialize AI models with GPU support"""
        try:
            # Text Generation Model (using Vertex AI)
            if os.getenv("GOOGLE_CLOUD_PROJECT"):
                vertexai.init(project=os.getenv("GOOGLE_CLOUD_PROJECT"))
                self.models["text_generator"] = GenerativeModel("gemini-pro")
                logger.info("‚úÖ Vertex AI Gemini Pro model loaded")
            else:
                logger.warning("‚ö†Ô∏è Google Cloud project not configured for Vertex AI")

            # Local NLP models (if GPU available)
            if self.gpu_available["cuda_available"]:
                try:
                    # Sentiment Analysis
                    self.pipelines["sentiment"] = pipeline(
                        "sentiment-analysis",
                        model="cardiffnlp/twitter-roberta-base-sentiment-latest",
                        device=0
                    )

                    # Text Classification
                    self.pipelines["text_classification"] = pipeline(
                        "text-classification",
                        model="microsoft/DialoGPT-medium",
                        device=0
                    )

                    logger.info("‚úÖ Local NLP models loaded with GPU acceleration")

                except Exception as e:
                    logger.error(f"‚ùå Failed to load local models: {e}")
                    # Fallback to CPU
                    self.pipelines["sentiment"] = pipeline("sentiment-analysis")
                    self.pipelines["text_classification"] = pipeline("text-classification")

        except Exception as e:
            logger.error(f"‚ùå AI model initialization failed: {e}")

    def _setup_routes(self):
        """Setup FastAPI routes"""

        @self.app.get("/")
        async def root():
            return {
                "service": "AI GPU Development Server",
                "status": "running",
                "gpu_available": self.gpu_available["cuda_available"],
                "models_loaded": list(self.models.keys()),
                "pipelines_available": list(self.pipelines.keys())
            }

        @self.app.get("/health")
        async def health_check():
            """Health check endpoint"""
            return {
                "status": "healthy",
                "gpu_status": self.gpu_available,
                "timestamp": asyncio.get_event_loop().time()
            }

        @self.app.get("/api/ai/health")
        async def ai_health():
            """AI-specific health check"""
            return {
                "ai_services": "operational",
                "gpu_acceleration": self.gpu_available["cuda_available"],
                "models_ready": len(self.models) + len(self.pipelines)
            }

        @self.app.post("/api/ai/generate")
        async def generate_text(request: Dict[str, Any]):
            """Generate text using AI models"""
            prompt = request.get("prompt", "")
            model_type = request.get("model", "vertex_ai")

            try:
                if model_type == "vertex_ai" and "text_generator" in self.models:
                    response = self.models["text_generator"].generate_content(prompt)
                    return {"response": response.text}

                elif model_type == "local" and "text_classification" in self.pipelines:
                    response = self.pipelines["text_classification"](prompt)
                    return {"response": response[0]}

                else:
                    raise HTTPException(status_code=400, detail="Model not available")

            except Exception as e:
                logger.error(f"Text generation error: {e}")
                raise HTTPException(status_code=500, detail=str(e))

        @self.app.post("/api/ai/sentiment")
        async def analyze_sentiment(request: Dict[str, Any]):
            """Analyze sentiment of text"""
            text = request.get("text", "")

            try:
                if "sentiment" in self.pipelines:
                    result = self.pipelines["sentiment"](text)
                    return {"sentiment": result[0]}
                else:
                    raise HTTPException(status_code=400, detail="Sentiment model not available")

            except Exception as e:
                logger.error(f"Sentiment analysis error: {e}")
                raise HTTPException(status_code=500, detail=str(e))

        @self.app.get("/api/ai/gpu/info")
        async def gpu_info():
            """Get detailed GPU information"""
            if not self.gpu_available["cuda_available"]:
                raise HTTPException(status_code=404, detail="No GPU available")

            info = {
                "device_count": torch.cuda.device_count(),
                "current_device": torch.cuda.current_device(),
                "device_name": torch.cuda.get_device_name(),
                "memory_allocated": torch.cuda.memory_allocated(),
                "memory_reserved": torch.cuda.memory_reserved(),
                "max_memory_reserved": torch.cuda.max_memory_reserved()
            }

            return info

        @self.app.post("/api/ai/todo/enhance")
        async def enhance_todo(request: Dict[str, Any]):
            """Enhance todo items with AI insights"""
            todo_text = request.get("text", "")
            enhancement_type = request.get("type", "suggestions")

            try:
                if "text_generator" in self.models:
                    if enhancement_type == "suggestions":
                        prompt = f"Suggest improvements for this todo item: '{todo_text}'"
                    elif enhancement_type == "breakdown":
                        prompt = f"Break down this todo into smaller tasks: '{todo_text}'"
                    else:
                        prompt = f"Analyze this todo item: '{todo_text}'"

                    response = self.models["text_generator"].generate_content(prompt)
                    return {
                        "original": todo_text,
                        "enhancement": response.text,
                        "type": enhancement_type
                    }
                else:
                    raise HTTPException(status_code=400, detail="AI enhancement not available")

            except Exception as e:
                logger.error(f"Todo enhancement error: {e}")
                raise HTTPException(status_code=500, detail=str(e))

    def run(self, host: str = "0.0.0.0", port: int = 3001):
        """Run the AI development server"""
        logger.info(f"üöÄ Starting AI GPU Development Server on {host}:{port}")
        logger.info(f"üî• GPU Acceleration: {'Enabled' if self.gpu_available['cuda_available'] else 'Disabled'}")

        uvicorn.run(self.app, host=host, port=port, reload=True)

# Global server instance
ai_server = AIDevelopmentServer()

if __name__ == "__main__":
    ai_server.run()
